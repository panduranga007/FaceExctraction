{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Four channel Face output- Face extracted using dlib,openCV</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the image name(Along with extension):\n",
      "gate.jpg\n",
      "\n",
      "Number of faces Detected:1\n",
      "Face:1\n",
      "Facial Landmarks:\n",
      "[[115 157]\n",
      " [112 238]\n",
      " [115 261]\n",
      " [120 285]\n",
      " [127 307]\n",
      " [141 326]\n",
      " [159 343]\n",
      " [181 356]\n",
      " [207 359]\n",
      " [233 356]\n",
      " [254 343]\n",
      " [270 326]\n",
      " [282 307]\n",
      " [288 286]\n",
      " [292 264]\n",
      " [297 241]\n",
      " [298 218]\n",
      " [301 157]]\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "#from imutils import face_utils\n",
    "import numpy as np\n",
    "#from PIL import Image\n",
    "\n",
    "def rectangle(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    # return a tuple of (x, y, w, h) (x,y)->Top-left corner w->width of rect h->depth of rect\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "\n",
    "load = str(input(\"Enter the image name(Along with extension):\\n\"))\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "image = cv2.imread(load)\n",
    "\n",
    "#convert from RBG to grayscale i.e.,PRE-PROCESS\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# dlib frontal face detector is used to find the faces\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "#to predict the 68 landmark features on the face- from which we use first 17(jaw line) landmarks\n",
    "predictor_path = \"dlib\\shape_predictor_68_face_landmarks.dat\"\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "face_rects = detector(gray, 1) \n",
    "\n",
    "#prints Number of detected faces\n",
    "print(\"\\nNumber of faces Detected:\"+str(len(face_rects)))\n",
    "\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(face_rects):\n",
    "    \n",
    "    #returned values are stored in tuple top-left  corner and width and height\n",
    "    (x1, y1, w, h) = rectangle(rect)\n",
    "    \n",
    "    #make a new black image on which we stick our cropped face this is of input image dimensions \n",
    "    out_face = np.zeros_like(image)\n",
    "    \n",
    "    \n",
    "    #prints each detected face top-left corners and width and height\n",
    "    print(\"Face:\"+str(i+1))\n",
    "    \n",
    "    #landmark predictor given a image and a face\n",
    "    shape = predictor(gray, rect)\n",
    "    \n",
    "    #finding the landmark points and manually give top two edges\n",
    "    pts = np.empty([18, 2], dtype = int)\n",
    "    for j in range(18):\n",
    "        if j == 0:\n",
    "            pts[j][0] = x1\n",
    "            pts[j][1] = y1\n",
    "        elif j == 17:\n",
    "            pts[j][0] = x1+w\n",
    "            pts[j][1] = y1\n",
    "        else:\n",
    "            pts[j][0] = shape.part(j).x\n",
    "            pts[j][1] = shape.part(j).y\n",
    "    \n",
    "    #constructing a mask\n",
    "    remapped_shape = np.zeros_like(shape)\n",
    "    image_mask = np.zeros((image.shape[0],image.shape[1]))\n",
    "    \n",
    "    #array of landmark pts\n",
    "    print('Facial Landmarks:')\n",
    "    print(pts)\n",
    "    \n",
    "    #marking the crop region in mask\n",
    "    cv2.fillPoly(image_mask,[np.array(pts, dtype=np.int32)],1)\n",
    "    image_mask = image_mask.astype(np.bool)\n",
    "    \n",
    "    #inserting that mask on output image\n",
    "    out_face[image_mask] = image[image_mask]\n",
    "    \n",
    "    #splitting the channels\n",
    "    b_channel, g_channel, r_channel = cv2.split(out_face)\n",
    "    \n",
    "    #initializing alpha channel\n",
    "    alpha_channel = np.zeros_like(gray)\n",
    "    \n",
    "    #assigning opaque value at image existing pixels\n",
    "    for p in range(alpha_channel.shape[0]):\n",
    "        for q in range(alpha_channel.shape[1]):\n",
    "            if b_channel[p][q]!=0 or g_channel[p][q]!=0 or r_channel[p][q]!=0:\n",
    "                alpha_channel[p][q] = 255\n",
    "                \n",
    "    #merging the four channels\n",
    "    output_face = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
    "    \n",
    "    #outputting the dark background face cropped image \n",
    "    string = \"output\"+str(i+1)+\".png\"\n",
    "    cv2.imwrite(string, output_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
